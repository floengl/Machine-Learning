\documentclass[a4paper,12pt]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}

\usepackage{colortbl} % For coloring table cells and lines
\usepackage[table]{xcolor} % For more color options


\geometry{a4paper, margin=1in}

% Title and Author
\title{Exercise 1}
\author{Your Name}

\begin{document}

% Title Page
\maketitle

% Abstract
\begin{abstract}
This is the abstract of the document. It provides a brief summary of the content.
\end{abstract}

% Table of Contents
\tableofcontents
\newpage

% Sections
\section{Introduction}
This is the introduction section. Here you can introduce the topic of your document.

\section{Methodology}
This section describes the methodology used in your work.

\subsection{Subsection Example}
This is an example of a subsection.

\section{Preprocessing}
We will do four different measures to properly preprocess the data. These are: data imputation, encoding of categorical variables,
scaling of numerical values and dimensionality reduction. In this section we will look at each dataset and 
describe which tasks can be used on the dataset and how a specific task is performed. Then we will compare the different
settings for all the models and determine their effectiveness using measures described in the previous section. 
In order to make this a meaningful comparison, we will use the default hyperparameters for all classifiers and 
we will split the data equally in the cross-validation.\\
Data leakage occurs when data is preprocessed before it was split into the training and test set. This way information 
from the test set is used to preprocess the training set. For example the MinMaxScaler uses the minimun and maximum values 
to scale these values properly. The values should only be computed on the training set and then be used on both training and 
test set. This can get rather difficult when using cross-validation. In order to avoid this, we will use the pipelines.    
Pipelines are a feature of scikit-learn that allow for the chaining of multiple transformers and estimators and also avoid 
data leakage. We will define a different pipeline for each preprocessing task and variant and chain these pipelines to compare 
the different settings.\\
\textbf{Congressional Voting:} As this dataset contains missing values we will need a strategy to impute these values. 
It is important to note that the missing values are all labeled as \textit{'unknown'}.
As all the values are categorical there are two possible strategies for data imputation.
We can either use the most frequent value of the respective attribute in the training dataset
or we can treat the missing values as a separate category. We will use one-hot-encoding for the categorical values,
except for the binary ones. Binary values will be encoded as 0 and 1. By using one-hot-encoding
the tow different treatments of the missing values will yield a different encoding. Still all the values remain binary, 
so no further scaling is needed. As the dataset is rather small, we will not use dimensionality reduction. This means
that we will only compare the two different strategies for data imputation.
In table~\ref{table:imputation} the results are visualized for the different classifiers.
Here the right number in each cell is the average accuracy and the left number is the average F1 score.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Data Imputation} & \textbf{Random Forest} & \textbf{LinearSVC} & \textbf{Ridge} \\
\hline
\textbf{most frequent} & \cellcolor[HTML]{C1E1C1} 0.957 /0.949  & \cellcolor[HTML]{C1E1C1} 0.962/0.954  & \cellcolor[HTML]{C1E1C1} 0.963 /0.957 \\
\hline
\textbf{nan is category} & 0.955 /0.947 & 0.955 /0.947& 0.960/0.953  \\
\hline
\end{tabular}
\caption{Comparison of data imputation for different classifiers for \textit{Congressional Voting} datasetW}
\label{table:imputation}
\end{table}
For every classifier the most frequent value imputation strategy yields better results for both accuracy and F1 score.
In this test we used 5 folds and 10 repetitions for the cross-validation.\\
\textbf{Amazon Commerce Reviews:} This dataset contains no missing values, therefore we don't need to impute any values.
The target attribute has 50 different values, which will be encoded as \textit{0,1,2\ldots,49}. 
All the other values are numerical, therefore we will try different scaling strategies.
We will try the most common scalers in the scikit-learn library, namely 
\begin{itemize}
    \item the MinMaxScaler which scales each feature to a given range, which is by default 0 to 1,
    \item the MaxAbsScaler which divides each feature by the maximum absolute values of the respective attribute,
    \item the StandardScaler which divides each feature by the standard deviation of the attribute,
    \item the RobustScaler which subtracts the median from each value and divides the result by the interquartile range of the respective attribute,
    \item the PowerTransformer which applies a power transformation to make the data more Gaussian-like,
    \item the QuantileTransformer which transforms the data to follow a uniform distribution.
\end{itemize}
The Transformers and the RobustScaler are known to be robust to outliers. We have also tested a method for dimensionality reduction called
principal component analysis (PCA). This method is used to reduce the number of features by projecting the data onto a lower dimensional space. 
The n\_components= 0.95 parameter determines, that our data should be projected onto the number of dimensions that explain 95\% of the variance.
Again the results are visualized in table~\ref{table:scaling}.



In the above Experiment we have used 5 folds and 10 repetitions for the cross-validation. The best results are achieved by the PowerTransformer for all classifiers. However, only the ridge classifier benefited from the
dimension reduction.



 \textbf{Census Income:} This dataset hat missing values, but only for categorical values.
 Those values are labeled with \textit{'?'}. We will try the same two strategies as for the \textit{Congressional Voting} dataset.
 The only exception is the \textit{'education'} column. This column is already encoded as the
 \textit{'education-num'} column and will therefore be dropped. For the numeric columns 
 we tested the same numerical scalers, which we have tried for the 
 \textit{Amazon Commerce Reviews} dataset. Since the dataset only has 14 features,
we will not use dimensionality reduction. Like for the previous
datasets we will present the result in a table, where the first score in each cell is accuracy and the second
number is the F1 score.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Scaler} & \textbf{Data Imputation} & \textbf{Random Forest} & \textbf{LinearSVC} & \textbf{Ridge} \\
\hline
StandardScaler & most frequent & \cellcolor[HTML]{C1E1C1} 0.857 / 0.677 & 0.852 / 0.655 & 0.841 / 0.607 \\
\hline
MaxAbsScaler & most frequent & 0.857 / 0.676 & 0.851 / 0.654 & 0.841 / 0.607 \\
\hline
None & most frequent & 0.857 / 0.676 & 0.800 / 0.379 & 0.830 / 0.567 \\
\hline
MinMaxScaler & most frequent & 0.857 / 0.676 & 0.852 / 0.654 & 0.841 / 0.607 \\
\hline
RobustScaler & most frequent & 0.857 / 0.676 & 0.851 / 0.649 & 0.830 / 0.561 \\
\hline
QuantileTransformer & most frequent & 0.857 / 0.676 & 0.845 / 0.645 & 0.842 / 0.621 \\
\hline
PowerTransformer & most frequent & 0.852 / 0.665 & 0.845 / 0.645 & 0.842 / 0.621 \\
\hline
MaxAbsScaler & \textit{'?'} is category & 0.857 / 0.676 & 0.853 / 0.659 & 0.841 / 0.610 \\
\hline
RobustScaler & \textit{'?'} is category & 0.857 / 0.676 & 0.852 / 0.658 & 0.841 / 0.610 \\
\hline
None & \textit{'?'} is category & 0.857 / 0.676 & 0.800 / 0.379 & 0.830 / 0.568 \\
\hline
StandardScaler & \textit{'?'} is category & 0.857 / 0.676 & \cellcolor[HTML]{C1E1C1}0.853 / 0.659 & 0.841 / 0.610 \\
\hline
MinMaxScaler & \textit{'?'} is category & 0.857 / 0.676 & 0.853 / 0.658 & 0.841 / 0.610 \\
\hline
QuantileTransformer & \textit{'?'} is category & 0.857 / 0.676 & 0.847 / 0.649 & \cellcolor[HTML]{C1E1C1} 0.844 / 0.627 \\
\hline
PowerTransformer & \textit{'?'} is category & 0.852 / 0.666 & 0.846 / 0.645 & 0.843 / 0.623 \\
\hline
\end{tabular}
\caption{Comparison of different numeric and categorical transformers for the \textit{Census Income} dataset}
\label{table:censustransformers}
\end{table}

We used 5 folds and 5 repetitions in the cross-validation for this experiment, except for the random forest
classifier. The random forest classifier was too slow, and we had to reduce the number of repetitions to 1.

 \textbf{Diabetes:} This dataset does not have any missing values except for the target attribute. Therefore, we 
 don't need to consider data imputation. As the dataset is very low dimensional, we will not use any dimensionality reduction. 
 So we will only compare different caling methods 







\section{Hyperparameters}
Now we want to take a closer look at the hyperparameters of our models. In a first step we want to optimize the hyperparameters for the different models and datasets.
In a second step we will analyze how sensitive the models are to changes in the hyperparameters. Unfortunately we will not be able to find the optimum for all hyperparameters, as scikit-learn offers a wide range, so we are going to focus on the most important ones.\\

For tuning we want to use Bayesian Optimization from the scikit-optimize package. Bayesian Optimization is a probabilistic approach to finding the minimum or maximum. This means that it builds a probabilistic model of the function and uses this model to decide on where to evaluate in the next step. In our case the function is given by the performance measure. 

Bayesian Optimization does not only take a function as input but a search space as well, which is why we need to decide on an appropriate rnage of values for the parameters as well. Moreover, because we chose two different performance measures for each dataset, we will have to do the optimization twice and then compare results to get the overall best.





\section{Discussion}
This section discusses the implications of your results.

\section{Conclusion}
This is the conclusion section. Summarize your findings and suggest future work.

% References
\begin{thebibliography}{9}
\bibitem{example1}
Author, \textit{Title of the Book}, Publisher, Year.

\bibitem{example2}
Author, \textit{Title of the Article}, Journal, Volume, Page numbers, Year.
\end{thebibliography}

\end{document}